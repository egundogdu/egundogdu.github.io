<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">

	a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 25px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 18px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <title>Erhan Gundogdu</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  </head>
  <body>
  <table width="1000" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Erhan Gundogdu, Ph.D.</name>
        </p>
        <p align="justify" style = "font-size:20px">I have been a Computer Vision Scientist at <a style = "font-size:20px" target="_blank" href="https://www.amazon.jobs/en/locations/berlin-germany">Amazon Berlin</a> since October 2019. I worked as a Postdoctoral Researcher at Computer Vision Laboratory (<a style = "font-size:20px" target="_blank" href="https://cvlab.epfl.ch/">CVLab</a>), &#201cole Polytechnique F&#233d&#233rale de Lausanne (EPFL) from March 2018 until October 2019.
        </p>
        <p style = "font-size:20px">
          I received my B.Sc., M.Sc. and Ph.D. degrees at Middle East Technical University, Turkey. During my M.Sc. and Ph.D. studies, 
          I was advised by <a style = "font-size:20px" target="_blank" href="http://users.metu.edu.tr/alatan/">Prof. Dr. A. Ayd&#305n Alatan</a>.
        </p>
        </td>
	    <td width="33%">
        <img src="IMG-1033.jpg" width="300" height="400">
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading style = "font-size:30px"><b>Research Interests</b></heading>
          <p style = "font-size:20px">
          My research interests include but not limited to: Computer Vision, Deep Learning, Machine Learning, Visual Object Tracking,
          Fine-grained Object Recognition, Deep Metric Learning, Signal Processing and Optimization.
          <p style = "font-size:20px"> For my full publication list, please visit <a style = "font-size:20px" target="_blank" href="https://scholar.google.ch/citations?user=nZD_5vsAAAAJ&hl=en&oi=ao">my Google Scholar Page</a>.
            My Ph.D. thesis is about visual object tracking (<a style = "font-size:20px" target="_blank" href="http://etd.lib.metu.edu.tr/upload/12621448/index.pdf">lib.metu</a>) and my M.Sc. thesis is about local feature detection and description learning for fast image matching (<a style = "font-size:20px" target="_blank" href="https://etd.lib.metu.edu.tr/upload/12614618/index.pdf">lib.metu</a>)   
          <p style = "font-size:20px"> My active interests are garment virtualization on 3D body shapes (at CVLAB, EPFL), visual object tracking (for my Ph.D. thesis) and fine-grained object recognition (at ASELSAN Research Center, Turkey).
          </p>
        </td>
      </tr>
      </table>
      


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading style = "font-size:20px"><b>Honors and Awards</b></heading>
            <ul>
              <li style = "font-size:15px"> <a target="_blank" style = "font-size:15px" href="http://sites.ieee.org/turkey/2018-yili-ieee-turkiye-bilim-odulleri-sahiplerini-buldu/">IEEE Turkey Ph.D. Thesis Award 2018</a> </li>
              
              <li style = "font-size:15px">The winning tracker award of <a style = "font-size:15px" target="_blank" href="http://www.votchallenge.net/">VOT2017</a> Visual Object Tracking Challenge in IEEE International Conference on Computer Vision (ICCV) 2017</li>
              
              <li style = "font-size:15px"> <a style = "font-size:15px" target="_blank" href="http://parlar.org.tr/2017-yili-odulleri/"> Mustafa Parlar Ph.D. Thesis Award</a> of the Year 2017</li>
              
              <li style = "font-size:15px"> Alper Atalay Student Paper Award in IEEE Signal Processing and Communications Applications <a style = "font-size:15px" target="_blank" href="http://siu2017.itu.edu.tr/en/index.html">(SIU) </a> 2017 </li>
            </ul>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="100%" valign="middle">
            <heading style = "font-size:30px"><b>Selected Projects</b></heading>
          </td>
        </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
      <td width="20%">
      <img src='uv_space.png' width="300" height="200">
      </td>
      <td valign="top" width="75%">
      <heading>Shape Reconstruction</heading><br>
      <papertitle>Shape Reconstruction by Learning Differentiable Surface Representations</papertitle>
      <br>
      (<a target="_blank" href="https://arxiv.org/pdf/1911.11227.pdf">arXiv Preprint</a>)<br>
      J. Bednarik, S. Parashar, <strong>E. Gundogdu</strong>, M. Salzmann, P. Fua,
      <em>accepted to IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020 <br>
      <p></p>
      <p id="textAreaSHAPE" align="justify" style = "font-size:15px">In this paper, we show that we can exploit the inherent differentiability of deep networks to leverage differential surface properties during training so as to prevent patch collapse and strongly reduce patch overlap.
      </p><a id="toggleButtonSHAPE" onclick="toggleTextSHAPE()" href="javascript:void(0);">See More</a>
      </td>
      </tr>
      </table>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>

      <td width="25%">
      <video width="300" class="border" controls loop>
      <source src="FTNN_full.mp4" type="video/mp4">
      </video><p></p><p></p>
      <video width="300" class="border" controls loop>
      <source src="MJNN_full.mp4" type="video/mp4">
      </video>
      </td>

      <td valign="top" width="75%">
      <heading>3D Cloth Draping by Deep Learning</heading><br>
      <papertitle>GarNet: A Two-stream Network for Fast and Accurate 3D Cloth Draping</papertitle> 
      (<a target="_blank" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Gundogdu_GarNet_A_Two-Stream_Network_for_Fast_and_Accurate_3D_Cloth_ICCV_2019_paper.pdf">thecvf.com</a>, <a target="_blank" href="https://arxiv.org/abs/1811.10983">arXiv Preprint</a>)<br>
      <strong>E. Gundogdu</strong>, V. Constantin, A. Seifoddini, M. Dang, M. Salzmann, P. Fua, 
      <em>IEEE International Conference on Computer Vision</em>, 2019 <br>
      <a target="_blank" href="garnet.bib">bibtex</a>, <a target="_blank" href="https://cvlab.epfl.ch/research/garment-simulation/garnet/">webpage</a>
      <p></p>
      <p id="textAreaGAR" align="justify" style = "font-size:15px"> While Physics-Based Simulation (PBS) can accurately drape a 3D garment on a 3D body, it remains too costly for real-time applications, such as virtual try-on. By contrast, inference in a deep network, requiring a single forward pass, is much faster. Taking advantage of this, we propose a novel architecture to fit a 3D garment template to a 3D body. 
      </p><a id="toggleButtonGAR" onclick="toggleTextGAR()" href="javascript:void(0);">See More</a>
      </td>
      </tr>
      </table>




      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
      <td width="25%">
      <img src='visuals.png' width="300" height="200">
      </td>
      <td valign="top" width="75%">
      <heading>Deep Learning for Correlation Filters</heading><br>
      <papertitle>Good Features to Correlate for Visual Tracking</papertitle>
      <br>
      (<a target="_blank" href="https://ieeexplore.ieee.org/document/8291524/">ieee.org</a>,
      <a target="_blank" href="https://arxiv.org/pdf/1704.06326.pdf">arXiv Preprint</a>)<br>
      <strong>E. Gundogdu</strong>, A. A. Alatan,
      <em>IEEE Transactions on Image Processing</em>, 2018 <br>
      <a target="_blank" href="https://github.com/egundogdu/CFCF">code</a>
      <a target="_blank" href="CFCF.bib">bibtex</a>
      <p></p>
      <p align="justify" style = "font-size:15px">In this work, the problem of learning deep fully convolutional features for the
        CFB visual tracking is formulated. To learn the proposed model, a novel and efficient backpropagation algorithm is presented
        based on the loss function of the network. The proposed learning framework enables the network model to be flexible
        for a custom design. Moreover, it alleviates the dependency on the network trained for classification. The proposed tracking method is the winner of
        <a target="_blank" href="http://www.votchallenge.net/">VOT2017</a> Challenge, organized by IEEE ICCV 2017.</p>
      </td>
      </tr>
      </table>
      
      
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
      <td width="25%">
        <img src='ensemble.png' width="300" height="200">
        <img src='spatialWindowing.png' width="300" height="200">  
      </td>
      <td valign="top" width="75%">
      <heading>Improving Correlation Filters</heading><br>
      <ul>
      <li><papertitle>Extending Correlation Filter based Visual Tracking by Tree-Structured Ensemble and Spatial Windowing</papertitle> (<a target="_blank" href="https://ieeexplore.ieee.org/document/7995133/">ieee.org</a>)<br>
        <strong>E. Gundogdu</strong>, H. Ozkan, A. A. Alatan,
        <em>IEEE Transactions on Image Processing</em>, 2017 <br>   
      </li><li><papertitle>Spatial Windowing for Correlation Filter Based Visual Tracking</papertitle> (<a target="_blank" href="https://ieeexplore.ieee.org/document/7532645/">ieee.org</a>)<br>
        <strong>E. Gundogdu</strong>, A. A. Alatan,
        <em>IEEE International Conference on Image Processing (ICIP), 2016</em> <br>
      </li><li><papertitle>Ensemble of Adaptive Correlation Filters for Robust Visual Tracking</papertitle> (<a target="_blank" href="https://ieeexplore.ieee.org/document/7738031/">ieee.org</a>)<br>
        <strong>E. Gundogdu</strong>, H. Ozkan, A. A. Alatan,
        <em>IEEE International Conference on Advanced Video and Signal-based Surveillance (AVSS), 2016</em> <br>
      </li></ul>
      <a target="_blank" href="ENSEMBLE.bib">bibtex</a>
      <p></p>
      
    	<p align="justify" style = "font-size:15px">In the studies above, we improve upon the conventional correlation filters by proposing two methods. First, we present an approach to learn a spatial window at each frame during the course of the tracking. When the learned window is element-wise multiplied by the object patch/correlation filter, it can suppress the irrelevant regions of the object patch. Second, a tree-structured ensemble of trackers algorithm is proposed to combine multiple correaltion filter-based trackers while hierarchically keeping the appearance model of the object at the tree nodes. At each frame, only the relevant node trackers are activated to be combined as the final tracking decision. The combination of these two approaches also yield a better performance.</p>
      </td>
      </tr>
      </table>   
      
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
      <td width="25%">
        <img src='MarvelDataset.jpg' width="300" height="200">
      </td>
      <td valign="top" width="75%">
      <heading>Visual Recognition for Maritime Vessels</heading><br>
      <ul>
      <li><papertitle>MARVEL: A Large-Scale Image Dataset for Maritime Vessels</papertitle> (<a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-319-54193-8_11">SpringerLink</a>)<br>
        <strong>E. Gundogdu</strong>, B. Solmaz, V. Yucesoy, A. Koc,
        <em>Asian Conference on Computer Vision</em>, 2016 <br>
      </li>
      <li><papertitle>Generic and Attribute-specific Deep Representations for Maritime Vessels </papertitle>(<a target="_blank" href="https://ipsjcva.springeropen.com/articles/10.1186/s41074-017-0033-4">SpringerOpen</a>)<br>
       B. Solmaz, <strong>E. Gundogdu</strong>, V. Yucesoy, A. Koc,
        <em>IPSJ Transactions on Computer Vision and Applications, 2017</em> <br>
	    </li>
      <li><papertitle>Fine-Grained Recognition of Maritime Vessels and Land Vehicles by Deep Feature Embedding </papertitle>(<a target="_blank" href="http://digital-library.theiet.org/content/journals/10.1049/iet-cvi.2018.5187">IET Digital Lib.</a>)<br>
       B. Solmaz, <strong>E. Gundogdu</strong>, V. Yucesoy, A. Koc, A. A. Alatan,
        <em>IEEE, IET Computer Vision, 2018</em> <br>
      </li>
      </ul>
      <a target="_blank" href="VESSELS.bib">bibtex</a>
      / 
      <a target="_blank" href="https://github.com/avaapm/marveldataset2016">dataset page</a>
      <p></p>
      
    	<p id="textAreaVES" align="justify" style = "font-size:15px">In the studies above, we first construct a large-scale maritime vessel dataset by distilling 2M annotated vessel images. Based on a semi-supervised clustering scheme, 26 hyper-classes for vessel types are construced. Four potential applications are introduced; namely, vessel classification, verification, retrieval and recognition with their provided baseline results.
      </p> <a id="toggleButtonVES" onclick="toggleTextVES()" href="javascript:void(0);">See More</a>
      </td>
      </tr>
      </table>
      
      
     <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
      <td width="25%">
        <img src='InfraredFeats.png' width="300" height="200">
        <img src='TBoost.png' width="300" height="200">  
      </td>
      <td valign="top" width="75%">
      <heading>Tracking and Recognition in Infrared Spectrum</heading><br>

      <ul>
      <li><papertitle>Comparison of Infrared and Visible Imagery for Object Tracking: Toward Trackers with Superior IR Performance</papertitle> (<a target="_blank" href="http://openaccess.thecvf.com/content_cvpr_workshops_2015/W05/papers/Gundogdu_Comparison_of_Infrared_2015_CVPR_paper.pdf">thecvf.com</a>)<br>
        <strong>E. Gundogdu</strong>, H. Ozkan, H. S. Demir, H. Ergezer, E. Akagunduz, S. K. Pakin<br>
        <em>IEEE Computer Vision and Pattern Recognition Workshops</em>, 2015 <br>   
      </li>
      <li><papertitle>Object classification in infrared images using deep representations</papertitle> (<a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/7532521/">ieee.org</a>)<br>
        <strong>E. Gundogdu</strong>, A. Koc, A. A. Alatan <br>
        <em>IEEE International Conference on Image Processing (ICIP), 2016</em> <br>
      </li>
      <li><papertitle>Evaluation of Feature Channels for Correlation-Filter-Based Visual Object Tracking in Infrared Spectrum</papertitle> (<a target="_blank" href="http://openaccess.thecvf.com/content_cvpr_2016_workshops/w9/papers/Gundogdu_Evaluation_of_Feature_CVPR_2016_paper.pdf">thecvf.com</a>)<br>
        <strong>E. Gundogdu</strong>, A. Koc, B. Solmaz, R. I. Hammoud, A. A. Alatan<br>
        <em>IEEE Computer Vision and Pattern Recognition Workshops</em>, 2016 <br>
      </li>
      </ul>
      <a target="_blank" href="INFRARED.bib">bibtex</a>
      <p></p>
      
    	<p id="textAreaIR" align="justify" style = "font-size:15px">Unlike the visible spectrum, the problem of object recognition and tracking are not extensively studied in Infrared (IR) Spectrum. In these studies, we first provide the first benchmark comparison work where the available tracking methods are evaluated in IR and Visible pairs of 20 videos and a novel ensemble of trackers method is presented.
      </p> <a id="toggleButtonIR" onclick="toggleTextIR()" href="javascript:void(0);">See More</a>
      </td>
      </tr>
      </table>  

    </td>
    </tr>
</table>

<script>
var statusIR = "less";

function toggleTextIR()
{
    var text="Unlike the visible spectrum, the problem of object recognition and tracking are not extensively studied in Infrared (IR) Spectrum. In these studies, we first provide the first benchmark comparison work where the available tracking methods are evaluated in IR and Visible pairs of 20 videos and a novel ensemble of trackers method is presented. Second, a deep learning based classification network is trained in an in-house dataset (consisting of more than 70 real-world IR videos) to learn IR specific features. Finally, these IR specific features are utilized for IR object tracking, and a significant amount of performance increase is observed with respect to the manually designed features of visible spectrum.";

    if (statusIR == "less") {
        document.getElementById("textAreaIR").innerHTML=text;
        document.getElementById("toggleButtonIR").innerHTML = "See Less";
        statusIR = "more";
    } else if (statusIR == "more") {
        document.getElementById("textAreaIR").innerHTML = "Unlike the visible spectrum, the problem of object recognition and tracking are not extensively studied in Infrared (IR) Spectrum. In these studies, we first provide the first benchmark comparison work where the available tracking methods are evaluated in IR and Visible pairs of 20 videos and a novel ensemble of trackers method is presented.";
        document.getElementById("toggleButtonIR").innerHTML = "See More";
        statusIR = "less"
    }
}

var statusVES = "less";

function toggleTextVES()
{
    var text="In the above studies, we first construct a large-scale maritime vessel dataset by distilling 2M annotated vessel images. Based on a semi-supervised clustering scheme, 26 hyper-classes for vessel types are construced. Four potential applications are introduced; namely, vessel classification, verification, retrieval and recognition with their provided baseline results. Furthermore, we attempted interesting problems of visual marine surveillance such as predicting and classifying maritime vessel attributes such as length, summer deadweight, draught, and gross tonnage by solely interpreting the visual content in the wild, where no additional cues such as scale, orientation, or location are provided. By utilizing generic and attribute-specific deep representations for maritime vessels, we obtained promising results for the aforementioned applications.";

    if (statusVES == "less") {
        document.getElementById("textAreaVES").innerHTML=text;
        document.getElementById("toggleButtonVES").innerHTML = "See Less";
        statusVES = "more";
    } else if (statusVES == "more") {
        document.getElementById("textAreaVES").innerHTML = "In the above studies, we first construct a large-scale maritime vessel dataset by distilling 2M annotated vessel images. Based on a semi-supervised clustering scheme, 26 hyper-classes for vessel types are construced. Four potential applications are introduced; namely, vessel classification, verification, retrieval and recognition with their provided baseline results.";
        document.getElementById("toggleButtonVES").innerHTML = "See More";
        statusVES = "less"
    }
}

var statusGAR = "less";

function toggleTextGAR()
{
    var text="While Physics-Based Simulation (PBS) can accurately drape a 3D garment on a 3D body, it remains too costly for real-time applications, such as virtual try-on. By contrast, inference in a deep network, requiring a single forward pass, is much faster. Taking advantage of this, we propose a novel architecture to fit a 3D garment template to a 3D body. Specifically, we build upon the recent progress in 3D point cloud processing with deep networks to extract garment features at varying levels of detail, including point-wise, patch-wise and global features. We fuse these features with those extracted in parallel from the 3D body, so as to model the cloth-body interactions. The resulting two-stream architecture, which we call as GarNet, is trained using a loss function inspired by physics-based modeling, and delivers visually plausible garment shapes whose 3D points are, on average, less than 1 cm away from those of a PBS method, while running 100 times faster. Moreover, the proposed method can model various garment types with different cutting patterns when parameters of those patterns are given as input to the network.";

    if (statusGAR == "less") {
        document.getElementById("textAreaGAR").innerHTML=text;
        document.getElementById("toggleButtonGAR").innerHTML = "See Less";
        statusGAR = "more";
    } else if (statusGAR == "more") {
        document.getElementById("textAreaGAR").innerHTML = "While Physics-Based Simulation (PBS) can accurately drape a 3D garment on a 3D body, it remains too costly for real-time applications, such as virtual try-on. By contrast, inference in a deep network, requiring a single forward pass, is much faster. Taking advantage of this, we propose a novel architecture to fit a 3D garment template to a 3D body.";
        document.getElementById("toggleButtonGAR").innerHTML = "See More";
        statusGAR = "less"
    }
}

var statusSHAPE = "less";

function toggleTextSHAPE()
{
    var text="Generative models that produce point clouds have emerged as a powerful tool to represent 3D surfaces, and the best current ones rely on learning an ensemble of parametric representations. Unfortunately, they offer no control over the deformations of the surface patches that form the ensemble and thus fail to prevent them from either overlapping or collapsing into single points or lines. As a consequence, computing shape properties such as surface normals and curvatures becomes difficult and unreliable. In this paper, we show that we can exploit the inherent differentiability of deep networks to leverage differential surface properties during training so as to prevent patch collapse and strongly reduce patch overlap. Furthermore, this lets us reliably compute quantities such as surface normals and curvatures. We will demonstrate on several tasks that this yields more accurate surface reconstructions than the state-of-the-art methods in terms of normals estimation and amount of collapsed and overlapped patches.";

    if (statusSHAPE == "less") {
        document.getElementById("textAreaSHAPE").innerHTML=text;
        document.getElementById("toggleButtonSHAPE").innerHTML = "See Less";
        statusSHAPE = "more";
    } else if (statusSHAPE == "more") {
        document.getElementById("textAreaSHAPE").innerHTML = "In this paper, we show that we can exploit the inherent differentiability of deep networks to leverage differential surface properties during training so as to prevent patch collapse and strongly reduce patch overlap.";
        document.getElementById("toggleButtonSHAPE").innerHTML = "See More";
        statusSHAPE = "less"
    }
}

</script>


</body>
</html>
